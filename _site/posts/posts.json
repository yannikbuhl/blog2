[
  {
    "path": "posts/2021-06-13-automated-reporting-on-lacking-rainfall-in-germany/",
    "title": "Automated Reporting on Lacking Rainfall in Germany",
    "description": "Climate change causes severe disturbances in what we used to call a more or less stable climate. Data journalists are, thus, increasingly focusing on quantifying the effects of the climate crisis. This is an example on how to do so.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2021-06-13",
    "categories": [],
    "contents": "\nClimate change causes severe disturbances in what we used to call a more or less stable climate. Germany, just as many other countries around the globe, suffers from an increasing lack of rainfall, which in turn causes situations close to what is called a draught. Data journalists are, thus, increasingly focusing on quantifying the effects of the climate crisis. When I used to work as an Editor at Stuttgarter Zeitung, I contributed to this journalistic goal by writing and automating a script that would help us tell the readers which region of Baden-Württemberg, Germany, has suffered from the longest absence of rainfall.\nIt is a more or less easy way of automating reports on the climate’s effects on our weather. In the following, I demonstrate how my script looks like. The data come from the German National Weather Service (DWD, Deutscher Wetterdienst).\nFirst, what you need is a bunch of packages:\n\nlibrary(pacman)\npacman::p_load(rdwd, magrittr, dplyr, here, bit64, lubridate, \n               readxl, purrr, backports, remotes)\n\n\nremotes::install_github(\"munichrocker/DatawRappr\")\nlibrary(DatawRappr)\n\nSecond, you need a list of all the DWD stations (including their IDs) that provide the desired measures (in this case, amount of rainfall, but this could also be temperature, etc.):\n\n# Set path\npath <- \"_posts/2021-06-13-automated-reporting-on-lacking-rainfall-in-germany/\"\n\n# Get stations\nstationen <- readxl::read_excel(here::here(path, \"stationen_bw.xlsx\"))\nhead(stationen)\n# A tibble: 6 x 6\n  stations_id stationshoehe stationsname          lufttemperatur  wind\n        <dbl>         <dbl> <chr>                          <dbl> <dbl>\n1          11           680 Donaueschingen Lande…              0     1\n2        1013           309 Dogern                             0     1\n3        1346          1490 Feldberg/Schwarzwald               0     1\n4        1443           237 Freiburg                           0     1\n5        1468           797 Freudenstadt                       0     1\n6        1490           394 Friedrichshafen                    0     1\n# … with 1 more variable: niederschlag <dbl>\n\nFor an analysis on rainfall, we need to extract those stations that measure it:\n\n# Get all stations\nniederschlag <- stationen$stations_id[stationen$niederschlag == 1]\n\nIn case you want to automate this script, create a folder where you want to store the data that the {rdwd} package downloads:\n\n# Check if folder for zip data exists & create it if necessary\nif (dir.exists(here::here(path, \"wetter\")) == FALSE) {\n  dir.create(here::here(path, \"wetter\"))}\n\nAfter these initial steps, we can start to download and process recent rainfall data from all stations involved. I download the data on a daily resolution first. Doing so, we can simply count the days since the last rainfall:\n\n# Get URLs for DWD zip files\ndownload <- rdwd::selectDWD(id = niederschlag, \n                            res = \"daily\", \n                            outvec = TRUE, \n                            var = \"more_precip\", \n                            per = \"recent\")\n\n# Download actual zip files and extract data as lists\nres1 <- rdwd::dataDWD(url = download, \n                      dir = here::here(path, \"wetter\"), \n                      force = TRUE, \n                      quiet = TRUE, \n                      overwrite = TRUE)\n\n# Create general data frame from all lists\nres1 %>% purrr::map_dfr(as.data.frame) -> res2\nres2 %>% filter(MESS_DATUM > as.POSIXct(\"2020-01-01\")) -> res2\n\n# Process and find last day of niederschlag\nresults <- res2 %>% group_by(STATIONS_ID) %>% \n  mutate(MESS_DATUM = as.POSIXct(MESS_DATUM)) %>% \n  filter(RS > 0) %>% \n  summarise(niederschlag = last(RS), time = last(MESS_DATUM)) %>% \n  mutate(days = as.integer(Sys.Date() - as.Date(time))) %>% \n  arrange(desc(days)) %>% \n  rename(stations_id = STATIONS_ID) %>% \n  mutate(time = format(time, format = \"%d.%m.%Y\"))\n\n# Check if folder with zip files exists and delete if present\nif (dir.exists(here::here(path, \"wetter\")) == TRUE) {\n      unlink(here::here(path, \"wetter\"), recursive = TRUE)\n}\n\nWhat is necessary now is to check whether there has been rainfall on this present day. If so, we have to set the counter of days without rain to 0:\n\nrecent1 <- rdwd::selectDWD(id = niederschlag, \n                           res = \"10_minutes\", \n                           outvec = TRUE, \n                           var = \"precipitation\", \n                           per = \"now\")\n\nif (dir.exists(here::here(path, \"wetter\")) == FALSE) {\n  dir.create(here::here(path, \"wetter\"))}\n\n# Download actual zip files and extract data as lists\nrecent2 <- rdwd::dataDWD(url = recent1, \n                         dir = here::here(path, \"wetter\"), \n                         force = TRUE, \n                         quiet = TRUE, \n                         overwrite = TRUE)\n\n# Create general data frame from all lists\nrecent2 %>% purrr::map_dfr(as.data.frame) -> recent3\n\n# Delete unused columns and delete all data before yesterday to prevent errors\nrecent3 %>% filter(MESS_DATUM > Sys.Date() - 1) %>% \n  dplyr::select(STATIONS_ID, MESS_DATUM, RWS_10) -> recent3\n\n# Wrangle data frame with weather data to get last value\nplausible <- recent3 %>% group_by(STATIONS_ID) %>% \n  summarise(sum = sum(RWS_10)) %>% \n  filter(sum > 0)\n\nif (sum(results$stations_id %in% plausible$STATIONS_ID) != 0) {\n  \n  for (i in seq_along(plausible$STATIONS_ID)) {\n    \n    id <- plausible$STATIONS_ID[i]\n    \n    results$time[results$stations_id == id] <- \n      format(Sys.Date(), format = \"%d.%m.%Y\")\n    \n    results$days[results$stations_id == id] <- 0\n    \n    results$niederschlag[results$stations_id == id] <- \n      plausible$sum[plausible$STATIONS_ID == id]\n    \n    rm(id)\n    \n  }\n  \n}\n\nresults <- results %>% arrange(desc(days))\nresults_regen <- results %>% arrange(desc(niederschlag))\n\nWhat is left now is to create the final data set:\n\n# Get names and station information\nstationen2 <- stationen %>% filter(niederschlag == 1) %>% \n  select(-lufttemperatur, -niederschlag, -wind)\n\n# Join weather dataset with stations dataset to get stations' names\nresults2 <- inner_join(results, stationen2, by = \"stations_id\") %>% \n  select(stationsname, days, niederschlag, time, stationshoehe) %>% \n  mutate(days = paste(days, \"Tag/en\")) %>% \n  rename(Station = stationsname, \n         `Letzter Niederschlag vor` = days,\n         `Niederschlag (in Litern)` = niederschlag,\n         `Letzter Niederschlag` = time,\n         `Stationshöhe (Meter)` = stationshoehe)\n\nif (dir.exists(here::here(path, \"wetter\")) == TRUE) {\n  unlink(here::here(path, \"wetter\"), recursive = TRUE)\n}\n\nThe result looks like this:\n\nhead(results2)\n# A tibble: 6 x 5\n  Station     `Letzter Niedersch… `Niederschlag (in… `Letzter Nieders…\n  <chr>       <chr>                            <dbl> <chr>            \n1 Dietenheim  8 Tag/en                           4   07.06.2021       \n2 Ihringen    7 Tag/en                          13.9 08.06.2021       \n3 Konstanz    7 Tag/en                          20.7 08.06.2021       \n4 Stuttgart … 7 Tag/en                           0.7 08.06.2021       \n5 Aulendorf-… 7 Tag/en                           0.8 08.06.2021       \n6 Baden-Bade… 6 Tag/en                           2.1 09.06.2021       \n# … with 1 more variable: Stationshöhe (Meter) <dbl>\n\nAs a data journalist, I sent these data to a Datawrapper chart (which is essentially a table) that displays all weather stations involved, the days since the last rainfall and the amount of rainfall the last time it rained:\n\n# Capture output which tells the URL of the chart\ncapture.output(dw_data_to_chart(x = results2, chart_id = \"abcdef\"), \n               file = \"/dev/null\")\n\n## Send data to datawrapper\nlog <- capture.output(dw_publish_chart(chart_id = \"abcdef\"))\n\nOn a side note, my R script triggers a python script on my Raspberry Pi if an error occurred and no valid URL was put out; the script will send an E-mail notifying me of the error:\n\n# If there was no valid URL in the output, trigger python script.\nif (grepl(pattern = \"https://datawrapper.dwcdn.net/abcdef\", log[6]) == FALSE) {\n\n    system(\"python3 ~Documents/scripts/duerre.py\")\n\n} else {\n\n    print(paste0(log[6], \" //// \", Sys.time()))\n\n}\n\nThis whole script runs on my Raspberry Pi 3 using a Cronjob.\n\n\n",
    "preview": {},
    "last_modified": "2021-06-15T19:53:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-23-automatically-filling-a-sqlite-database-using-aws-and-rstudio-server/",
    "title": "Automatically filling a SQLite database using AWS and RStudio Server",
    "description": "The R script used is a simple parser. To automate it, set up a SQLite database on AWS. The script parses a JSON file and writes it into the database using SQL.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-05-12",
    "categories": [],
    "contents": "\nFor about two years now, I measure the air quality (more specifically: fine dust) on my balcony. Up to now, the data were stored by an Open Data project and displayed (more or less hidden) in the internet – at least you would need to know the name or ID of the sensor to access the data as very basic graphs. Here you could also find aggregated air quality data for all areas of Stuttgart. Recently, I began storing the data into a database myself to be able to do my own calculations (and eventually create a Shiny App that would display nicer, interactive graphs). Here, I wanted to very quickly write down what I did and what is going on.\nThe sensor\nThe sensor measuring fine dust is a very basic one. All its components cost around 30€ and can be shopped online. Honestly, I did not build the sensor myself; a colleague had a spare one and gave it to me. However, I was told it is not too hard to build such a sensor. In fact, here you can find a (German only) manual for doing so, including a list of stuff you need for the sensor itself. My sensor is located on my balcony, which in turn faces a highly frequented road, where quite a lot of buses are passing by during the day. The sensor delivers the data it measures every (more or less) five minutes via a JSON file to an API that is open for everyone to read. The sensor itself runs quite smoothly, only recently I run into problems where it ceases to send information – this is fixed after quickly removing it from its power source.\nAmazon AWS with RStudio Server\nWhen I came up with the idea of automatically storing the data into a database, I was initially thinking about setting up a Raspberry Pi to do the job. Yet, a friend recommended using Amazon AWS instead. Frankly, when I did this, I was new to cloud computing. However, there are a lot of tutorials out there on how to set up an Amazon AWS instance. I remember choosing the smallest one (EC2) which is also completely free of charge. That is useful since it is supposed to run 24/7. Whilst setting up the instance, I also installed RStudio Server and Shiny Server. That would later allow me to work on R scripts on the instance in order to do the parsing of the JSON file.\nThe script\nThe R script is nothing more than a simple parser for the JSON file that the API publishes. As a first step, I set up a SQLite database on my AWS instance using the R package sqldf. It contains only three variables: the timestamp of the measurement and the two fine dust measurements that the sensor gives us. The script parses the JSON file (using jsonlite) and writes it into the database using SQL. Here is some example code:\n\n\ndb <- dbConnect(SQLite(), dbname = \"/home/user/database.sqlite\")\nquery1 <- paste0(\"INSERT INTO database VALUES ('\", time1, \"',\", PM_10_1, \",\", PM_25_1, \")\")\ndbSendQuery(conn = db, query1)\n\n\n\nSetting up the Crontab\nThe last step is to automate the R script that we have stored on the AWS instance, too. You can do this using a Crontab, which is an automation tool that is already present on the Linux of the EC2 instance. You can find a tutorial here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-23T00:09:05+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-introducing-the-r-newsanchor-package/",
    "title": "Introducing the R {newsanchor} package",
    "description": "At CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs using newsapi.org.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-03-06",
    "categories": [],
    "contents": "\nAt CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike:* {newsanchor}, CorrelAid’s first open source R package. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs - using the API of newsapi.org. I have co-authored the package and I am thrilled to introduce the package here, too.\nThe (mostly free) News API is one way to access text as a resource for data analyses. It provides news articles and breaking news from a variety of sources across various countries, delivered to the analyst via an API (HTTP REST). Users are offered three API endpoints: top headlines, everything, and sources. Top headlines provides access to live breaking headlines of the news sources in a given country. Everything outputs articles published by these sources on a specified search query, even back in the past. Sources helps users to get access to the set of news sources that are available to top headlines.\nAll search requests come with different meta data (URL, author, date of publication, topic, etc.) and can be refined by a huge variety of additional parameters (sources, topic, time, relevance, etc.). For more details, see newsapi.org. Note for German scientists and journalists: In Germany, the following sources are available: Spiegel Online, Handelsblatt, Tagesspiegel, Wired, Gründerszene, BILD, Zeit Online, Focus Online, t3n and Wirtschaftswoche.\nAfter a short registration, the API can be accessed via code: through client libraries such as JavaScript or Ruby. But until now, there has been no R package that does the work (or search) conveniently. Now, at CorrelAid, a team of five data analysts developed this package. The package is called {newsanchor} and is available on CRAN: install.packages(\"newsanchor\").\nNewsanchor provides three functions that correspond to the API’s endpoints: get_headlines(), get_everything() and get_sources(). They help users to conveniently scrape the resources of News API, specify different search parameters and obtain results as 1) a data frame with results and 2) a list with meta data on the search. We also provide comprehensive error messages to make troubleshooting easy. You find details on the usage of newsanchor and its core functions in our general CRAN vignette.\nAnother reason for us to develop the package was that analyses based on words are becoming increasingly important. Political scientists, for example, classify parties on any ideological dimension using party manifestos. Other scholars focus on news articles to extract the (political) framings of the texts. Using automatisation, it is, for example, possible to calculate the sentiment of a given text fragment such as, for instance, online commentaries. The resulting data prove useful both as a dependent variable as well as an independent variable of any further analysis.\nThe importance of text analyses arises from the origin of ‘texts’: People aim at a certain reaction of their readers. Among the producers of texts with most influence are the media: newspapers, online magazines or blogs. By publishing articles, opinion pieces and analyses, they shape public opinion. The topics they choose (or not choose), the words they use, the quantity of articles on a certain issue – all these factors make them a worthy basis of investigation.\nAs already mentioned, an example would be to calculate the sentiment of news articles. Newsanchor can help to filter and scrape texts from news sources. In our example code, our co-developer Jan Dix shows you how to do so by getting URLs of the New York Times with newsanchor::get_everything(), subsequently scraping them with {httr} and analysing the articles’ sentiments.\nWe hope {newsanchor} will help scientists, journalists and other data enthusiasts to start scraping and using text data based on news articles.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-22T23:53:07+02:00",
    "input_file": {}
  }
]
