[
  {
    "path": "posts/2022-07-30-gas-storage-data-introducing-the-giedata-package-for-r/",
    "title": "Gas Storage Data: Introducing the {giedata} Package for R",
    "description": "With gas storage data gaining increased attention due to the consequences of Russia's war against Ukraine, I have written a package to access the API of Gas Infrastructure Europe (GIE).",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2022-07-30",
    "categories": [],
    "contents": "\nIt has not been until Russia’s brutal attack on Ukraine in February\nof 2022 that daily data on gas storage facilities in Europe would be an\ninteresting topic to deal with for a broader public. Yet, in response to\nthe sanctions put in place by the European Union, Russia has reduced the\nflow of natural gas to EU countries (thereby exploiting their huge\ndependency on Russian gas, probably for political reasons). In many\ncountries, it is now questionable as to whether the gas storage\nfacilities will hold enough gas to get through winter time.\nThis is why politicians, journalists, managers and statisticians are\nnow in need for high frequency, reliable data on gas storage facilities.\nIn Europe, they are provided by Gas\nInfrastructure Europe (GIE). For their AGSI+/ALSI+ transparency\nplatform they offer an API that provides all available data on gas\nstorage, the most important data being filling level as well as inflow\nand outflow.\nRight after people started searching for gas storage data with the\nstart of the war, GIE changed the architecture of their API\nfundamentally, introducing pagination and updating most endpoints and\nparameters. Since there was no pre-existing R package that could work\nwith the new architecture, I decided to write it myself. As a result,\n{giedata} was published on CRAN in July 2022. Here, I want\nto give a short overview of what it offers:\nThe first step in a possible workflow would be to fetch the metadata\nof the available gas storage companies and their facilities in a given\nregion or country. In this example, I will fetch all gas storage\ncompanies in Germany including their facilities, using the first of\nthree main functions of the package, get_gielisting():\n\n\nstorage <- giedata::get_gielisting(region = \"Europe\",\n                                   country = \"Germany\",\n                                   facilities = TRUE)\n\nhead(storage, n = 5)\n\n# A tibble: 5 × 9\n  facility_eic     facility_name     facility_type company_eic country\n  <chr>            <chr>             <chr>         <chr>       <chr>  \n1 21Z000000000271O UGS Rehden        Storage Faci… 21X0000000… Germany\n2 21W0000000001148 UGS Jemgum H (as… Storage Faci… 21X0000000… Germany\n3 21W0000000001261 VSP NORD (Rehden… Storage Group 21X0000000… Germany\n4 21W0000000000184 UGS Wolfersberg   Storage Faci… 37X0000000… Germany\n5 21W0000000001083 UGS Berlin        Storage Faci… 37X0000000… Germany\n# … with 4 more variables: country_code <chr>,\n#   company_shortname <chr>, company_name <chr>, company_url <chr>\n\nBased on this information, we can then use the function\nget_giedata() to download the data from AGSI+/ALSI+ (ALSI+\nfor liquefied natural gas is not yet supported for the new API\narchitecture). From the data set above we see that the unique ID (EIC)\nof the company “astora” is 21X000000001160J, we can use\nthat now (from the data set above we, too, get the EIC of the storage\nunit in Rehden):\n\n\ndata <- giedata::get_giedata(country = \"DE\",\n                             company = \"21X000000001160J\",\n                             facility = \"21Z000000000271O\", \n                             from = \"2022-01-01\",\n                             to = \"2022-01-05\")\n\nhead(data)\n\n# A tibble: 5 × 13\n  name       code  url   gasDayStart gasInStorage injection withdrawal\n  <chr>      <chr> <chr> <date>             <dbl>     <dbl>      <dbl>\n1 UGS Rehden 21Z0… 21Z0… 2022-01-01          2.86     229.         0  \n2 UGS Rehden 21Z0… 21Z0… 2022-01-02          3.00     138.         0  \n3 UGS Rehden 21Z0… 21Z0… 2022-01-03          3.08      79.7        0  \n4 UGS Rehden 21Z0… 21Z0… 2022-01-04          3.16      86.1        0  \n5 UGS Rehden 21Z0… 21Z0… 2022-01-05          3.15       0         12.6\n# … with 6 more variables: workingGasVolume <dbl>,\n#   injectionCapacity <dbl>, withdrawalCapacity <dbl>, status <chr>,\n#   trend <dbl>, full <dbl>\n\nThe last of three main functions is get_giedata2() and\nis a generalised version of get_giedata() that allows you\nto download data for multiple countries, companies or facilities at once\nso you do not have to loop over get_giedata() yourself:\n\n\ndata2 <- giedata::get_giedata2(countries = c(\"DE\", \"NL\", \"AT\"),\n                               date = \"2022-07-01\")\n\nhead(data2)\n\n# A tibble: 3 × 15\n  name        code  url   gasDayStart gasInStorage consumption\n  <chr>       <chr> <chr> <date>             <dbl>       <dbl>\n1 Germany     DE    DE    2022-07-01         149.        995. \n2 Netherlands NL    NL    2022-07-01          75.6       420. \n3 Austria     AT    AT    2022-07-01          43.2        98.1\n# … with 9 more variables: consumptionFull <dbl>, injection <dbl>,\n#   withdrawal <dbl>, workingGasVolume <dbl>,\n#   injectionCapacity <dbl>, withdrawalCapacity <dbl>, status <chr>,\n#   trend <dbl>, full <dbl>\n\nNote that due to the design of the API the functionality of\nget_giedata2() is - as of yet - a tiny bit complicated: You\ncan specify multiple countries and get the data on the country level.\nOnce you want to get data on companies, you can only specify one\ncountry, and all EIC codes provided must be of this country’s origin.\nThe same holds for facilities: You can get data for mulitple facilities\nat once, but the country and company EIC must be of length one.\nCase 1: Country + companies\n\n\ndata3 <- giedata::get_giedata2(countries = \"DE\",\n                               companies = c(\"21X000000001160J\", \n                                             \"37X0000000000151\"),\n                               date = \"2022-01-01\")\n\nhead(data3)\n\n# A tibble: 2 × 13\n  name       code  url   gasDayStart gasInStorage injection withdrawal\n  <chr>      <chr> <chr> <date>             <dbl>     <dbl>      <dbl>\n1 astora (G… 21X0… 21X0… 2022-01-01          8.28      241.        0.5\n2 bayernugs  37X0… 37X0… 2022-01-01          1.82        0        10.5\n# … with 6 more variables: workingGasVolume <dbl>,\n#   injectionCapacity <dbl>, withdrawalCapacity <dbl>, status <chr>,\n#   trend <dbl>, full <dbl>\n\nCase 2: Company + facilities\n\n\ndata4 <- giedata::get_giedata2(countries = \"DE\",\n                               companies = \"21X000000001160J\",\n                               facilities = c(\"21Z000000000271O\", \n                                              \"21W0000000001148\"),\n                               date = \"2022-01-01\")\n\nhead(data4)\n\n# A tibble: 2 × 13\n  name       code  url   gasDayStart gasInStorage injection withdrawal\n  <chr>      <chr> <chr> <date>             <dbl>     <dbl>      <dbl>\n1 UGS Rehden 21Z0… 21Z0… 2022-01-01          2.86     229.         0  \n2 UGS Jemgu… 21W0… 21W0… 2022-01-01          5.42      11.5        0.5\n# … with 6 more variables: workingGasVolume <dbl>,\n#   injectionCapacity <dbl>, withdrawalCapacity <dbl>, status <chr>,\n#   trend <dbl>, full <dbl>\n\nLastly, I hope this package is of use to you and if you have any\nquery or suggestion, do not hesitate and hop over to Github and\ncreate an issue.\n\n\n\n",
    "preview": {},
    "last_modified": "2022-08-07T00:30:46+02:00",
    "input_file": "gas-storage-data-introducing-the-giedata-package-for-r.knit.md"
  },
  {
    "path": "posts/2021-12-21-telling-stories-with-data-insights-into-data-journalism/",
    "title": "Telling Stories with Data: Insights into Data Journalism",
    "description": "Telling stories with data is one of the most important things to do for almost everyone working with data analyses. Why? Because its goal is reaching the audience one wants to reach.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2021-12-21",
    "categories": [],
    "contents": "\nNote: This is a cross post by courtesy of the MZES Social Science Data Lab, Mannheim.\nTelling stories with data is one of the most important things to do for almost everyone working with data analyses. Why? Because its goal is reaching the audience one wants to reach. If one succeeds therein – be it, for example, average news consumers or academics in a specific field –, the underlying data analysis will more probably have a lasting impact. A good story drags the audience into your analysis.\nThis is where data-driven journalism comes into play - a field in journalism that emerged a few years ago with advancing technological developments (such as easier web-scraping and advanced visualisation, programming, and the ability to process large amounts of data more easily, etc.) being introduced to newsrooms and journalistic curricula. Examples range from simple yet challenging data analysis with a line chart to visually and analytically more advanced stories. Even methods of machine learning have recently been used in newsrooms for investigative research. By outlining its background and principles, this post also shows how closely communicating scientific results is related to data journalism and what researchers can take away from it to deliver even more compelling insights.\nIn this Methods Bites Tutorial, I want to offer a recap of my workshop “Telling Stories with Data - Insights into Data Journalism” in the MZES Social Science Data Lab during Spring 2021. It focuses on the important steps to tell a thorough story based on data analyses – and how scientists and data journalists can learn from each other.\nThe original workshop materials, including slides, are available from the Lab’s GitHub. A live recording of the workshop is available on the Lab’s YouTube Channel.\nWhere do data stories originate?\nThere are two basic ways of finding a data-driven story in the first place: starting with the data or starting with the story. In general, it is desirable to have a story (and maybe even a potential headline) in mind when starting your research. This way, you most probably already have some kind of structure (or, say, hypotheses) that you can follow. Thus, in my opinion, it’s a more journalistic way to work, because you select your hypotheses already based on your perception of the key concept: relevance. Only then will you proceed to data research or collection. But that is of course not always how things work. Sometimes, you just stumble upon a new data set and you don’t know yet if there is a story inside. Maybe also some informant sends you a lot of unstructured data and you have to explore it yourself. In any case, in this scenario you start by exploring in a trial-and-error way, attempting to find a story while thinking of a relevant headline. Here, as well as generally, it is important to keep in mind that any correlation you may find does not mean there is also causation.\nWhat is a good data-driven story?\nBut what is a good story after all? In short, you have to bring the relevance of your analysis to the surface. By relevance I mean some sort of topic that is important for society or at least for a huge portion of your audience (here, it helps to have a good understanding of who your audience is). Relevance, especially in terms of data journalism, also means trying to question common assumptions that have not been investigated using data – a common relevance criterion for scientists as well. Your audience will read and process your (data-driven) story mainly if they understand the way it affects their personal life. In this context, it is always a good idea to actively underline this aspect in the headline and also during the first paragraph(s) of your text. In a data context, relevance often involves breaking down your data to a local level so readers find themselves, their relatives, or their environment in the data. This way, you make sure to definitely gain their attention. Let me give you an example: Some years ago, I wanted to know how many children up to three years go to daycare. Germany had enacted a law guaranteeing this for all children of that age, but from anecdotal evidence, it was evident that daycare institutions struggled to meet this goal - a huge problem for young families. I collected data on a very local level, thus covering a quite pressing societal problem, showcasing how the government failed its own goals. Additionally, I was enabling the readers to find information on their town or region in the data to tell whether they’re doing better or worse than others.\nSumming up, this means in order to tell a good data-driven story:\nThink of a headline (i.e., a precise key message) while doing your project so you keep focusing on what is important.\nThink of who your audience is and tell them straight away why your story is relevant in their daily lives.\nDiscuss your topic with colleagues to make sure your storyline is consistent.\nKnow the questions you want to ask the data and know what not to find in the data.\nIn data exploration terms: Transform and merge with other data, count or total some variables of interest, do comparisons and show change over time, etc.\nChallenges for data-driven journalism\nIn their day-to-day work, data journalists face some challenges. To keep things short, I want to set aside problems involving badly formatted data and other technological challenges such as poorly designed or undocumented APIs. But there are some general things you have to cope with during research for a news piece.\nFirst, to ensure the quality of the conclusions you draw from your analysis, you have to make sure you know the data generating process of the data set you are working with. Relevant questions to ask are for instance who collected the data and how? Do the data allow for the conclusions you draw (think of the variables’ operationalisation)? Is your data representative (e.g., has it been collected using randomization)? If you haven’t thought about this enough, critical readers will inevitably challenge your story.\nSecond, always double-check for potentially erroneous data. To do so, also talk to colleagues if possible. Do frequent plausibility checks. If your analysis involves code and you have enough time, set it aside for a day and come back to it later. Even better, if you have colleagues that can code, show and explain your coding. Alternatively, explain a non-coding colleague what you did step-by-step to identify mistakes.\nThird, do not forget classic journalistic research with data sources that are available for your specific topic. Always think about whether there are better alternatives to your current database. Explain in your text why this is the best source available.\nFourth, if you’re writing your news piece, don’t over-emphasise technological or methodological aspects, especially when you write for a general audience. Readers often don’t have a lot of time and what’s important is that they understand your key message or conclusion (i.e., the reason your headline dragged them into reading). The challenge here is to don’t become inaccurate or over-interpret your results at the same time. Oftentimes, there is little time in newsrooms to proof-read for this aspect, but it is inherently important for basic journalistic work.\nSome thoughts on data visualisation\nData visualisation is a whole chapter itself, and there are tons of excellent tutorials out there on how to make reader-friendly graphics, which is why I cannot possibly get too much into detail here (for every-day graphics in newsrooms think of the phantastic blog posts of Lisa Charlotte Muth of Datawrapper or the homepage of Information is Beautiful, for general visualization think of the books and work of Alberto Cairo). However, I’d like to share some general thoughts, because they are – besides a compelling key message – a crucial ingredient for your story:\n-Try to visualise your data if possible, many people understand information way better if it is accompanied by a visualisation. Also, people tend to stay longer with your article if they find graphics there. One example from my work is this explainer on how the parties in the European Parliament are grouped ideologically and by fraction (and how these two things sometimes do not overlap). Data visualisations, especially interactive ones, must not be too full of information. It is good practice to guide your reader through the graph – just as me and some colleagues at Süddeutsche Zeitung did in this visual piece on the political factions in the European Parliament. Using scrollytelling, some explanatory text and visual highlighting, we make sure the reader understands the most important message.\n© Süddeutsche Zeitung GmbH, München. Mit freundlicher Genehmigung von Süddeutsche Zeitung Content (www.sz-content.de)User experience is of utmost importance. If your readers need too much time or work to understand a visualisation, try to be more concise.\nMobile-first! Most readers consume your content on a mobile phone. So make sure that they can consume your graphics on these devices effortlessly.\nHowever: The story guides the visualisation. Don’t make graphs for the sake of it. If they create confusion, don’t visualise (maybe try a table instead). Better have no graphics than bad ones.\nThink about what the reader needs to know to understand your graph. This depends on your audience and their needs.\nIf possible, go local. Make an interactive visualisation where people can find themselves (i.e., their town, area, neighbourhood, etc.).\nIf you have decided to do data visualisations, rather include more charts in your text instead of one chart with too much information in it! Put differently: If there is too much information in one chart, try to split the information into different graphs. For example, many readers already fail to interpret a two-dimensional scatter plot immediately. If you have to spend more than two or three sentences to explain the graph, make it easier instead. There is, however, no general rule on how many charts a text can take – it largely depends on the length, type and topic of the text. The coverage of the Covid-19 pandemic yields a great example across the media. Here, a complex topic has to be broken down – e.g., by using small multiple graphs when going local.\nScientists and data journalism\nSumming up, I believe scientists can learn a lot from data journalism. They, too, want their results to be recognised by the general public. By - as far as possible - adhering to the principles laid down in this blog post, I believe scientists can increase their audience to some extent by thinking about its needs while writing up results, visualising, and finding a compelling story. At the same time, data journalists can learn from the scientific way of working, namely: explaining the boundaries of data analysis and communicating them. Another important thing is communicating uncertainty (i.e., things like confidence intervals and the concept of simulations/scenarios versus an actual prediction, already somehow common in election coverage and Covid-19 coverage). Sometimes, data journalistic projects reach scientific spheres. This is when concepts like reproducibility and transparency come into play for data journalists as well. And at this point, it helps to work together with scientists of the field of concern.\nFurther readings\nData Journalism Handbook 1\nData Journalism Handbook 2\n\n\n\n",
    "preview": {},
    "last_modified": "2022-07-30T20:14:45+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-13-automated-reporting-on-lacking-rainfall-in-germany/",
    "title": "Automated Reporting on Lacking Rainfall in Germany",
    "description": "Climate change causes severe disturbances in what we used to call a more or less stable climate. Data journalists are, thus, increasingly focusing on quantifying the effects of the climate crisis. This is an example on how to do so.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2021-06-13",
    "categories": [],
    "contents": "\nClimate change causes severe disturbances in what we used to call a more or less stable climate. Germany, just as many other countries around the globe, suffers from an increasing lack of rainfall, which in turn causes situations close to what is called a draught. Data journalists are, thus, increasingly focusing on quantifying the effects of the climate crisis. When I used to work as an Editor at Stuttgarter Zeitung, I contributed to this journalistic goal by writing and automating a script that would help us tell the readers which region of Baden-Württemberg, Germany, has suffered from the longest absence of rainfall.\nIt is a more or less easy way of automating reports on the climate’s effects on our weather. In the following, I demonstrate how my script looks like. The data come from the German National Weather Service (DWD, Deutscher Wetterdienst).\nFirst, what you need is a bunch of packages:\n\nlibrary(pacman)\npacman::p_load(rdwd, magrittr, dplyr, here, bit64, lubridate, \n               readxl, purrr, backports, remotes)\n\n\nremotes::install_github(\"munichrocker/DatawRappr\")\nlibrary(DatawRappr)\n\nSecond, you need a list of all the DWD stations (including their IDs) that provide the desired measures (in this case, amount of rainfall, but this could also be temperature, etc.):\n\n# Set path\npath <- \"_posts/2021-06-13-automated-reporting-on-lacking-rainfall-in-germany/\"\n\n# Get stations\nstationen <- readxl::read_excel(here::here(path, \"stationen_bw.xlsx\"))\nhead(stationen)\n# A tibble: 6 x 6\n  stations_id stationshoehe stationsname          lufttemperatur  wind\n        <dbl>         <dbl> <chr>                          <dbl> <dbl>\n1          11           680 Donaueschingen Lande…              0     1\n2        1013           309 Dogern                             0     1\n3        1346          1490 Feldberg/Schwarzwald               0     1\n4        1443           237 Freiburg                           0     1\n5        1468           797 Freudenstadt                       0     1\n6        1490           394 Friedrichshafen                    0     1\n# … with 1 more variable: niederschlag <dbl>\n\nFor an analysis on rainfall, we need to extract those stations that measure it:\n\n# Get all stations\nniederschlag <- stationen$stations_id[stationen$niederschlag == 1]\n\nIn case you want to automate this script, create a folder where you want to store the data that the {rdwd} package downloads:\n\n# Check if folder for zip data exists & create it if necessary\nif (dir.exists(here::here(path, \"wetter\")) == FALSE) {\n  dir.create(here::here(path, \"wetter\"))}\n\nAfter these initial steps, we can start to download and process recent rainfall data from all stations involved. I download the data on a daily resolution first. Doing so, we can simply count the days since the last rainfall:\n\n# Get URLs for DWD zip files\ndownload <- rdwd::selectDWD(id = niederschlag, \n                            res = \"daily\", \n                            outvec = TRUE, \n                            var = \"more_precip\", \n                            per = \"recent\")\n\n# Download actual zip files and extract data as lists\nres1 <- rdwd::dataDWD(url = download, \n                      dir = here::here(path, \"wetter\"), \n                      force = TRUE, \n                      quiet = TRUE, \n                      overwrite = TRUE)\n\n# Create general data frame from all lists\nres1 %>% purrr::map_dfr(as.data.frame) -> res2\nres2 %>% filter(MESS_DATUM > as.POSIXct(\"2020-01-01\")) -> res2\n\n# Process and find last day of niederschlag\nresults <- res2 %>% group_by(STATIONS_ID) %>% \n  mutate(MESS_DATUM = as.POSIXct(MESS_DATUM)) %>% \n  filter(RS > 0) %>% \n  summarise(niederschlag = last(RS), time = last(MESS_DATUM)) %>% \n  mutate(days = as.integer(Sys.Date() - as.Date(time))) %>% \n  arrange(desc(days)) %>% \n  rename(stations_id = STATIONS_ID) %>% \n  mutate(time = format(time, format = \"%d.%m.%Y\"))\n\n# Check if folder with zip files exists and delete if present\nif (dir.exists(here::here(path, \"wetter\")) == TRUE) {\n      unlink(here::here(path, \"wetter\"), recursive = TRUE)\n}\n\nWhat is necessary now is to check whether there has been rainfall on this present day. If so, we have to set the counter of days without rain to 0:\n\nrecent1 <- rdwd::selectDWD(id = niederschlag, \n                           res = \"10_minutes\", \n                           outvec = TRUE, \n                           var = \"precipitation\", \n                           per = \"now\")\n\nif (dir.exists(here::here(path, \"wetter\")) == FALSE) {\n  dir.create(here::here(path, \"wetter\"))}\n\n# Download actual zip files and extract data as lists\nrecent2 <- rdwd::dataDWD(url = recent1, \n                         dir = here::here(path, \"wetter\"), \n                         force = TRUE, \n                         quiet = TRUE, \n                         overwrite = TRUE)\n\n# Create general data frame from all lists\nrecent2 %>% purrr::map_dfr(as.data.frame) -> recent3\n\n# Delete unused columns and delete all data before yesterday to prevent errors\nrecent3 %>% filter(MESS_DATUM > Sys.Date() - 1) %>% \n  dplyr::select(STATIONS_ID, MESS_DATUM, RWS_10) -> recent3\n\n# Wrangle data frame with weather data to get last value\nplausible <- recent3 %>% group_by(STATIONS_ID) %>% \n  summarise(sum = sum(RWS_10)) %>% \n  filter(sum > 0)\n\nif (sum(results$stations_id %in% plausible$STATIONS_ID) != 0) {\n  \n  for (i in seq_along(plausible$STATIONS_ID)) {\n    \n    id <- plausible$STATIONS_ID[i]\n    \n    results$time[results$stations_id == id] <- \n      format(Sys.Date(), format = \"%d.%m.%Y\")\n    \n    results$days[results$stations_id == id] <- 0\n    \n    results$niederschlag[results$stations_id == id] <- \n      plausible$sum[plausible$STATIONS_ID == id]\n    \n    rm(id)\n    \n  }\n  \n}\n\nresults <- results %>% arrange(desc(days))\nresults_regen <- results %>% arrange(desc(niederschlag))\n\nWhat is left now is to create the final data set:\n\n# Get names and station information\nstationen2 <- stationen %>% filter(niederschlag == 1) %>% \n  select(-lufttemperatur, -niederschlag, -wind)\n\n# Join weather dataset with stations dataset to get stations' names\nresults2 <- inner_join(results, stationen2, by = \"stations_id\") %>% \n  select(stationsname, days, niederschlag, time, stationshoehe) %>% \n  mutate(days = paste(days, \"Tag/en\")) %>% \n  rename(Station = stationsname, \n         `Letzter Niederschlag vor` = days,\n         `Niederschlag (in Litern)` = niederschlag,\n         `Letzter Niederschlag` = time,\n         `Stationshöhe (Meter)` = stationshoehe)\n\nif (dir.exists(here::here(path, \"wetter\")) == TRUE) {\n  unlink(here::here(path, \"wetter\"), recursive = TRUE)\n}\n\nThe result looks like this:\n\nhead(results2)\n# A tibble: 6 x 5\n  Station     `Letzter Niedersch… `Niederschlag (in… `Letzter Nieders…\n  <chr>       <chr>                            <dbl> <chr>            \n1 Dietenheim  8 Tag/en                           4   07.06.2021       \n2 Ihringen    7 Tag/en                          13.9 08.06.2021       \n3 Konstanz    7 Tag/en                          20.7 08.06.2021       \n4 Stuttgart … 7 Tag/en                           0.7 08.06.2021       \n5 Aulendorf-… 7 Tag/en                           0.8 08.06.2021       \n6 Baden-Bade… 6 Tag/en                           2.1 09.06.2021       \n# … with 1 more variable: Stationshöhe (Meter) <dbl>\n\nAs a data journalist, I sent these data to a Datawrapper chart (which is essentially a table) that displays all weather stations involved, the days since the last rainfall and the amount of rainfall the last time it rained:\n\n# Capture output which tells the URL of the chart\ncapture.output(dw_data_to_chart(x = results2, chart_id = \"abcdef\"), \n               file = \"/dev/null\")\n\n## Send data to datawrapper\nlog <- capture.output(dw_publish_chart(chart_id = \"abcdef\"))\n\nOn a side note, my R script triggers a python script on my Raspberry Pi if an error occurred and no valid URL was put out; the script will send an E-mail notifying me of the error:\n\n# If there was no valid URL in the output, trigger python script.\nif (grepl(pattern = \"https://datawrapper.dwcdn.net/abcdef\", log[6]) == FALSE) {\n\n    system(\"python3 ~Documents/scripts/duerre.py\")\n\n} else {\n\n    print(paste0(log[6], \" //// \", Sys.time()))\n\n}\n\nThis whole script runs on my Raspberry Pi 3 using a Cronjob.\n\n\n",
    "preview": {},
    "last_modified": "2021-06-15T19:53:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-27-covid-19-see-german-shopping-streets-emptying/",
    "title": "Covid-19 - See German shopping streets emptying",
    "description": "The coronavirus is halting public live. Looking at pedestrian data shows how persistently people stay away from German city centres and give an estimate of the situation is for shops in city centres.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2020-03-28",
    "categories": [],
    "contents": "\nThe coronavirus is halting public live all over the world. So, many of the patterns of data regarding flight departures or traffic that we see these days are, objectively speaking, unsurprising. As of now, most of us are coping well with the situation and stay at home, as is recommended. As a result, the public sphere is emptying. Right now, this is a strategy that I support. However, it is hitting the economy harder than one might think. This is especially true for the small shops in the city centres. Most of them have been forced to close.\nSo, looking at pedestrian data can show us two things, anyway. 1) They show us how persistently people are keeping away from German city centres. And 2) they show us a rough estimate of how bad the situation is for the shops in the city centres. Maybe two years ago, I learned from a colleague that there is this new homepage where one could get pedestrian data from German cities, where they measure the amount of pedestrians on central German shopping streets using lasers. The site is called hystreet.com and its data can be downloaded freely after creating an account. Since the spread of Covid-19, some journalists have had a look at this kind of data already for the big German cities. In this tiny, quick hands-on analysis, I focus on the federal state of Baden-Württemberg, which is where I live.\nFirst, we need a couple of packages.\n\n\nlibrary(tidyverse, warn.conflicts = FALSE)\nlibrary(here)\nlibrary(lubridate)\n\n\n\nSecond, we need to do a bit of data wrangling. I start with loading two datasets that I downloaded from hystreet.com. They contain data for the largest shopping street in Stuttgart, the Königstraße. The dataset from 2020 contains data from January 6th until March 26th. I have a second one from 2019 that contains data from March 1st to March 31st.\n\n\n# Read the csv file from hystreet.com\n\npath <- paste0(here::here(), \n              \"/_posts/2021-06-27-covid-19-see-german-shopping-streets-emptying/\")\n\ndf1 <- read.csv(here::here(path, \"stuttgart-königstraße-mitte.csv\"), \n                sep = \";\", \n                stringsAsFactors = FALSE)\ndf2 <- read.csv(here::here(path, \"stuttgart-königstraße-mitte-2019.csv\"), \n                sep = \";\", \n                stringsAsFactors = FALSE)\n\nrm(path)\n\n\n\n\n\n# Some data preparation\ndf1 <- df1 %>% mutate(time_of_measurement = as.POSIXct(time_of_measurement),\n               date = lubridate::as_date(time_of_measurement),\n               time = lubridate::ceiling_date(time_of_measurement, unit = \"hours\"))\n\ndf2 <- df2 %>% mutate(time_of_measurement = as.POSIXct(time_of_measurement),\n               date = lubridate::as_date(time_of_measurement),\n               time = lubridate::ceiling_date(time_of_measurement, unit = \"hours\"))\n\n\n\nAfter doing so, the data set looks like this:\n\n\nglimpse(df1)\n\n\nRows: 30,934\nColumns: 7\n$ location            <chr> \"Königstraße (Mitte), Stuttgart\", \"König…\n$ time_of_measurement <dttm> 2020-01-06 00:04:21, 2020-01-06 00:09:5…\n$ counted_pedestrians <int> 31, 24, 34, 30, 32, 23, 23, 12, 27, 27, …\n$ type                <chr> \"regular\", \"regular\", \"regular\", \"regula…\n$ incidents           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ date                <date> 2020-01-06, 2020-01-06, 2020-01-06, 202…\n$ time                <dttm> 2020-01-06 01:00:00, 2020-01-06 01:00:0…\n\nhead(df1, n = 5)\n\n\n                        location time_of_measurement\n1 Königstraße (Mitte), Stuttgart 2020-01-06 00:04:21\n2 Königstraße (Mitte), Stuttgart 2020-01-06 00:09:56\n3 Königstraße (Mitte), Stuttgart 2020-01-06 00:14:15\n4 Königstraße (Mitte), Stuttgart 2020-01-06 00:19:58\n5 Königstraße (Mitte), Stuttgart 2020-01-06 00:24:37\n  counted_pedestrians    type incidents       date\n1                  31 regular        NA 2020-01-06\n2                  24 regular        NA 2020-01-06\n3                  34 regular        NA 2020-01-06\n4                  30 regular        NA 2020-01-06\n5                  32 regular        NA 2020-01-06\n                 time\n1 2020-01-06 01:00:00\n2 2020-01-06 01:00:00\n3 2020-01-06 01:00:00\n4 2020-01-06 01:00:00\n5 2020-01-06 01:00:00\n\nThe first thing I am interested in is the long-term perspective. First restrictions on public life have been announced by the federal government on March 17th (red line in the figure below), further restrictions followed on March 23rd (blue line in the figure below). The second figure shows a comparable time span from 2019. This way, we can (if only slightly) control for seasonal effects, too. That second time span of 2019 serves as a benchmark for how the number of pedestrians would have developed if the coronavirus restrictions would not have hit. I scaled the y axis equally to a maximum of 15,000 pedestrians per hour to spot the difference more easily.\n\n\ndf1 %>% filter(date >= \"2020-03-01\") %>% group_by(time) %>% \n  summarise(n = sum(counted_pedestrians)) %>%\n ggplot(aes(x = time, y = n)) + \n  geom_line(size = 1) +\n  geom_vline(xintercept = as.POSIXct(\"2020-03-17\"), color = \"red\") +\n  geom_vline(xintercept = as.POSIXct(\"2020-03-23\"), color = \"blue\") +\n  ggtitle(\"2020: Pedestrians in the Stuttgart city centre\", \n          subtitle = \"Cumulated number on Königstraße (Mitte)\") + \n  labs(caption = \"Source: hystreet.com\") + \n  xlab(\"Date\") + \n  ylab(\"Sum of pedestrians per hour\") +\n  ylim(0, 15000)\n\n\n\ndf2 %>% filter(date <= \"2019-03-26\") %>% group_by(time) %>% \n  summarise(n = sum(counted_pedestrians)) %>%\n ggplot(aes(x = time, y = n)) + \n  geom_line(size = 1) +\n  ggtitle(\"2019: Pedestrians in the Stuttgart city centre\", \n          subtitle = \"Cumulated number on Königstraße (Mitte)\") + \n  labs(caption = \"Source: hystreet.com\") + \n  xlab(\"Date\") + \n  ylab(\"Sum of pedestrians per hour\") + \n  ylim(0, 15000)\n\n\n\n\nYou can see that even before the restrictions were passed by the federal government, numbers of pedestrians were already in decline and are now almost absent. In raw numbers, this is (again, compared to 2019):\n\n\ndf1 %>% group_by(date) %>% \n  summarise(n = sum(counted_pedestrians)) %>% tail(., n = 7)\n\n\n# A tibble: 7 x 2\n  date           n\n  <date>     <int>\n1 2020-03-20  9908\n2 2020-03-21  2884\n3 2020-03-22  4350\n4 2020-03-23  6628\n5 2020-03-24  7564\n6 2020-03-25  6692\n7 2020-03-26   656\n\ndf2 %>% group_by(date) %>% \n  summarise(n = sum(counted_pedestrians)) %>% tail(., n = 7)\n\n\n# A tibble: 7 x 2\n  date            n\n  <date>      <int>\n1 2019-03-25  34788\n2 2019-03-26  40252\n3 2019-03-27  44181\n4 2019-03-28  50518\n5 2019-03-29  69944\n6 2019-03-30 148604\n7 2019-03-31  35709\n\nAs a last analysis, I want to focus on the weekends, unsurprisingly the days with most pedestrians in the city centres. This last figure basically speaks for itself. I compare two weekends end of March, 2020 versus 2019. It is evident 1) how many people stayed home (as long as they did not meet up in any other place instead) and 2) how many potential customers the city centre shops are missing every day (and especially Saturday) that they have to remain closed.\n\n\ntime1 <- seq.POSIXt(as.POSIXct(\"2020-03-20 00:00:00\"), \n                    as.POSIXct(\"2020-03-23 00:00:00\"), by = \"hour\")\n\ntime2 <- seq.POSIXt(as.POSIXct(\"2019-03-22 00:00:00\"), \n                    as.POSIXct(\"2019-03-25 00:00:00\"), by = \"hour\")\n\nplot1 <- df1 %>%\n   filter(time %in% time1) %>%\n   group_by(time) %>%\n   summarise(n = sum(counted_pedestrians)) %>%\n   mutate(hour = lubridate::hour(time),\n          date = lubridate::as_date(time),\n          day = lubridate::day(time),\n          id = 1:length(n))\n\nplot2 <- df2 %>%\n    filter(time %in% time2) %>%\n    group_by(time) %>%\n    summarise(n = sum(counted_pedestrians)) %>% \n    mutate(hour = lubridate::hour(time),\n          date = lubridate::as_date(time),\n          day = lubridate::day(time),\n          id = 1:length(n))\n\n plot1 %>% ggplot(aes(x = id, y = n)) + geom_line(size = 1, color = \"red\") +\n   ggtitle(\"Pedestrians on a March weekend 2020 (red) versus 2019\",\n           subtitle = \"Königstraße Stuttgart (Mitte)\") +\n   ylim(0, 15000)+\n   ylab(\"Number of pedestrians per hour\") +\n   xlab(\"Time in hours (start: Friday 00:00)\") +\n   geom_line(data = plot2) +\n   labs(caption = \"Source: hystreet.com\")\n\n\n\n\nThis is not the most insightful analysis since the figures that we can see were to expected given the current Covid-19 crisis. However, I think by using data from hystreet.com we can make a bit more evident how many people actually stayed away from the centres. I replicated the analysis for a couple of other cities of south west Germany – unsurprisingly, again, they all look pretty similar. If you continued up until this very end: Stay healthy – and please stay home (for now).\n\n\n\n",
    "preview": "posts/2021-06-27-covid-19-see-german-shopping-streets-emptying/covid-19-see-german-shopping-streets-emptying_files/figure-html5/plots-1.png",
    "last_modified": "2021-06-27T22:47:21+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-27-scraping-spiegel-articles-using-newsanchor/",
    "title": "Scraping Spiegel articles using {newsanchor}",
    "description": "One convenient use of {newsanchor} is to use it to scrape the articles' content. Our package is of great help because it provides you with the corresponding URLs.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2020-01-27",
    "categories": [],
    "contents": "\nAlmost a year ago, we (a team at CorrelAid) published our first open source package for R on CRAN: {newsanchor}. It queries the API of newsapi.org and allows you to easily download the articles relating to your search query of a range of popular international news outlets. Results include a variety of meta data, the URLs as well as (depending on whether you have a paid plan or not) parts of the article content. You can find all information needed reading its vignette. I also wrote an introductory blog post which you can find here.\nOne convenient use of {newsanchor} is to use it to scrape the articles’ content. Our package is of great help because it provides you with the corresponding URLs. It is fairly easy to build a scraper upon that. Here, I have to mention that vast parts of the following code stem from Jan Dix who, like me, co-authored the package. He wrote a scraper for the New York Times, it was originally to be included as a vignette of {newsanchor} (it had to be removed because of dependecy trouble, though). What I did was to build on his code and add another example for the popular German news magazine Spiegel.\n[Note: There is code for a progress bar in the following chunk. It is commented out so the R-Markdown output would be easier to read]\n\n\n# Load packages required\nlibrary(newsanchor) # download newspaper articles\nlibrary(robotstxt)  # get robots.txt\nlibrary(httr)       # http requests\nsuppressMessages(library(rvest))      # web scraping tools\nsuppressMessages(library(dplyr))      # easy data frame manipulation\nlibrary(stringr)    # string/character manipulation \n# Get headlines published by SPON using newsanchor (example)\nresponse <- get_everything_all(query   = \"Merkel\",\n                                 sources = \"spiegel-online\",\n                                 from    = Sys.Date() - 3,\n                                 to      = Sys.Date())\n  \n# Extract response data frame\narticles <- response$results_df\n\n# Check robots.txt if scraping is OK\nsuppressMessages(allowed <- paths_allowed(articles$url))\nall(allowed)\n\n# Define parsing function\nget_article_body <- function (url) {\n  \n  # Download article page\n  response <- GET(url)\n  \n  # Check if request was successful\n  if (response$status_code != 200) return(NA)\n  \n  # Extract HTML\n  html <- content(x        = response, \n                  type     = \"text\", \n                  encoding = \"UTF-8\")\n  \n  # Parse html\n  parsed_html <- read_html(html)                   \n  \n  # Define paragraph DOM selector\n  selector <- \"div.clearfix p\"\n  \n  # Parse content\n  parsed_html %>% \n    html_nodes(selector) %>%      # extract all paragraphs within class 'article-section'\n    html_text() %>%               # extract content of the <p> tags\n    str_replace_all(\"\\n\", \"\") %>% # replace all line breaks\n    paste(collapse = \" \")         # join all paragraphs into one string\n}\n\n# Apply function to all URLs\n# Create new text column\narticles$body <- NA\n# Initialize progress bar\n# pb <- txtProgressBar(min     = 1, \n#                      max     = nrow(articles), \n#                      initial = 1, \n#                      style   = 3)\n\n# Loop through articles and apply function\nfor (i in 1:nrow(articles)) {\n  \n  # Apply function to i in URLS\n  articles$body[i] <- get_article_body(articles$url[i])\n  \n  ## Update progress bar\n  # setTxtProgressBar(pb, i)\n  \n  # Sleep for 1 sec\n  Sys.sleep(1)\n}\n\n\n\nBased on the articles’ content, you can, for example, compute sentiment analyses. Jan shows you how to do that for the New York Times in what was formerly our vignette here. I hope this is useful for some of you.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-28T17:28:22+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-23-automatically-filling-a-sqlite-database-using-aws-and-rstudio-server/",
    "title": "Automatically filling a SQLite database using AWS and RStudio Server",
    "description": "The R script used is a simple parser. To automate it, set up a SQLite database on AWS. The script parses a JSON file and writes it into the database using SQL.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-05-12",
    "categories": [],
    "contents": "\nFor about two years now, I measure the air quality (more specifically: fine dust) on my balcony. Up to now, the data were stored by an Open Data project and displayed (more or less hidden) in the internet – at least you would need to know the name or ID of the sensor to access the data as very basic graphs. Here you could also find aggregated air quality data for all areas of Stuttgart. Recently, I began storing the data into a database myself to be able to do my own calculations (and eventually create a Shiny App that would display nicer, interactive graphs). Here, I wanted to very quickly write down what I did and what is going on.\nThe sensor\nThe sensor measuring fine dust is a very basic one. All its components cost around 30€ and can be shopped online. Honestly, I did not build the sensor myself; a colleague had a spare one and gave it to me. However, I was told it is not too hard to build such a sensor. In fact, here you can find a (German only) manual for doing so, including a list of stuff you need for the sensor itself. My sensor is located on my balcony, which in turn faces a highly frequented road, where quite a lot of buses are passing by during the day. The sensor delivers the data it measures every (more or less) five minutes via a JSON file to an API that is open for everyone to read. The sensor itself runs quite smoothly, only recently I run into problems where it ceases to send information – this is fixed after quickly removing it from its power source.\nAmazon AWS with RStudio Server\nWhen I came up with the idea of automatically storing the data into a database, I was initially thinking about setting up a Raspberry Pi to do the job. Yet, a friend recommended using Amazon AWS instead. Frankly, when I did this, I was new to cloud computing. However, there are a lot of tutorials out there on how to set up an Amazon AWS instance. I remember choosing the smallest one (EC2) which is also completely free of charge. That is useful since it is supposed to run 24/7. Whilst setting up the instance, I also installed RStudio Server and Shiny Server. That would later allow me to work on R scripts on the instance in order to do the parsing of the JSON file.\nThe script\nThe R script is nothing more than a simple parser for the JSON file that the API publishes. As a first step, I set up a SQLite database on my AWS instance using the R package sqldf. It contains only three variables: the timestamp of the measurement and the two fine dust measurements that the sensor gives us. The script parses the JSON file (using jsonlite) and writes it into the database using SQL. Here is some example code:\n\n\ndb <- dbConnect(SQLite(), dbname = \"/home/user/database.sqlite\")\nquery1 <- paste0(\"INSERT INTO database VALUES ('\", time1, \"',\", PM_10_1, \",\", PM_25_1, \")\")\ndbSendQuery(conn = db, query1)\n\n\n\nSetting up the Crontab\nThe last step is to automate the R script that we have stored on the AWS instance, too. You can do this using a Crontab, which is an automation tool that is already present on the Linux of the EC2 instance. You can find a tutorial here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-23T00:16:10+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-introducing-the-r-newsanchor-package/",
    "title": "Introducing the R {newsanchor} package",
    "description": "At CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs using newsapi.org.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-03-06",
    "categories": [],
    "contents": "\nAt CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike:* {newsanchor}, CorrelAid’s first open source R package. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs - using the API of newsapi.org. I have co-authored the package and I am thrilled to introduce the package here, too.\nThe (mostly free) News API is one way to access text as a resource for data analyses. It provides news articles and breaking news from a variety of sources across various countries, delivered to the analyst via an API (HTTP REST). Users are offered three API endpoints: top headlines, everything, and sources. Top headlines provides access to live breaking headlines of the news sources in a given country. Everything outputs articles published by these sources on a specified search query, even back in the past. Sources helps users to get access to the set of news sources that are available to top headlines.\nAll search requests come with different meta data (URL, author, date of publication, topic, etc.) and can be refined by a huge variety of additional parameters (sources, topic, time, relevance, etc.). For more details, see newsapi.org. Note for German scientists and journalists: In Germany, the following sources are available: Spiegel Online, Handelsblatt, Tagesspiegel, Wired, Gründerszene, BILD, Zeit Online, Focus Online, t3n and Wirtschaftswoche.\nAfter a short registration, the API can be accessed via code: through client libraries such as JavaScript or Ruby. But until now, there has been no R package that does the work (or search) conveniently. Now, at CorrelAid, a team of five data analysts developed this package. The package is called {newsanchor} and is available on CRAN: install.packages(\"newsanchor\").\nNewsanchor provides three functions that correspond to the API’s endpoints: get_headlines(), get_everything() and get_sources(). They help users to conveniently scrape the resources of News API, specify different search parameters and obtain results as 1) a data frame with results and 2) a list with meta data on the search. We also provide comprehensive error messages to make troubleshooting easy. You find details on the usage of newsanchor and its core functions in our general CRAN vignette.\nAnother reason for us to develop the package was that analyses based on words are becoming increasingly important. Political scientists, for example, classify parties on any ideological dimension using party manifestos. Other scholars focus on news articles to extract the (political) framings of the texts. Using automatisation, it is, for example, possible to calculate the sentiment of a given text fragment such as, for instance, online commentaries. The resulting data prove useful both as a dependent variable as well as an independent variable of any further analysis.\nThe importance of text analyses arises from the origin of ‘texts’: People aim at a certain reaction of their readers. Among the producers of texts with most influence are the media: newspapers, online magazines or blogs. By publishing articles, opinion pieces and analyses, they shape public opinion. The topics they choose (or not choose), the words they use, the quantity of articles on a certain issue – all these factors make them a worthy basis of investigation.\nAs already mentioned, an example would be to calculate the sentiment of news articles. Newsanchor can help to filter and scrape texts from news sources. In our example code, our co-developer Jan Dix shows you how to do so by getting URLs of the New York Times with newsanchor::get_everything(), subsequently scraping them with {httr} and analysing the articles’ sentiments.\nWe hope {newsanchor} will help scientists, journalists and other data enthusiasts to start scraping and using text data based on news articles.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-23T00:16:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-27-voting-behaviour-in-the-2014-scottish-independence-referendum/",
    "title": "Voting Behaviour in the 2014 Scottish Independence Referendum",
    "description": "I investigated the voting behaviour of the Scottish electorate in the 2014 Scottish independence referendum. More precisely, I was the first attempting to explain voting behaviour in the referendum based on a multivariate quantitative approach.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-01-09",
    "categories": [],
    "contents": "\nFor my Master’s thesis, submitted in July 2017, I investigated the voting behaviour of the Scottish electorate in the 2014 Scottish independence referendum. More precisely, I was the first attempting to explain voting behaviour in the referendum based on a multivariate quantitative approach. In doing so, I additionally focused on Scots’ attitudes towards the European Union and how these attitudes influence(d) their behaviour at the ballots. Whilst the existing literature on the referendum mostly ignored the European dimension of the independence debate, my results suggest that the Scots did in fact somehow think about the future of Scotland within or outside of the EU when the cast their vote – and that these attitudes did have an effect on how they would decide.\nIn sum, my results are the following (ceteris paribus): Scots who found EU membership to be a good thing and who thought that an independent Scotland would not be able to remain in the EU had a comparatively low probability of voting in favour of independence. However, once they believed that independent Scotland would be able to remain an EU member, the probability of voting in favour of independence increased sharply. These effects (based on logistic regression controlling for all sorts of variables and attitudes such as age, gender, nationalism, party identificiation, etc.) become more easy to understand once they are presented in a graph using predicted probabilities (higher values of “Perceived Likelihood of Scotland retaining EU membership” mark higher confidence of the survey respondent): This relationship gets even more pronounced after computing an interaction effect and separating between proponents and opponents of EU membership: \nThis blog post was just meant to be as short presentation of my results. Here you can download the PDF with the full thesis.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-27T21:57:59+02:00",
    "input_file": {}
  }
]
