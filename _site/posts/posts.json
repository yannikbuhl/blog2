[
  {
    "path": "posts/2021-06-13-automated-reporting-on-lacking-rainfall-in-germany/",
    "title": "Automated Reporting on Lacking Rainfall in Germany",
    "description": "Climate change causes severe disturbances in what we used to call a more or less stable climate. Data journalists are, thus, increasingly focusing on quantifying the effects of the climate crisis. This is an example on how to do so.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2021-06-13",
    "categories": [],
    "contents": "\nClimate change causes severe disturbances in what we used to call a more or less stable climate. Germany, just as many other countries around the globe, suffers from an increasing lack of rainfall, which in turn causes situations close to what is called a draught. Data journalists are, thus, increasingly focusing on quantifying the effects of the climate crisis. When I used to work as an Editor at Stuttgarter Zeitung, I contributed to this journalistic goal by writing and automating a script that would help us tell the readers which region of Baden-Württemberg, Germany, has suffered from the longest absence of rainfall.\nIt is a more or less easy way of automating reports on the climate’s effects on our weather. In the following, I demonstrate how my script looks like. The data come from the German National Weather Service (DWD, Deutscher Wetterdienst).\nFirst, what you need is a bunch of packages:\n\nlibrary(pacman)\npacman::p_load(rdwd, magrittr, dplyr, here, bit64, lubridate, \n               readxl, purrr, backports, remotes)\n\n\nremotes::install_github(\"munichrocker/DatawRappr\")\nlibrary(DatawRappr)\n\nSecond, you need a list of all the DWD stations (including their IDs) that provide the desired measures (in this case, amount of rainfall, but this could also be temperature, etc.):\n\n# Set path\npath <- \"_posts/2021-06-13-automated-reporting-on-lacking-rainfall-in-germany/\"\n\n# Get stations\nstationen <- readxl::read_excel(here::here(path, \"stationen_bw.xlsx\"))\nhead(stationen)\n# A tibble: 6 x 6\n  stations_id stationshoehe stationsname          lufttemperatur  wind\n        <dbl>         <dbl> <chr>                          <dbl> <dbl>\n1          11           680 Donaueschingen Lande…              0     1\n2        1013           309 Dogern                             0     1\n3        1346          1490 Feldberg/Schwarzwald               0     1\n4        1443           237 Freiburg                           0     1\n5        1468           797 Freudenstadt                       0     1\n6        1490           394 Friedrichshafen                    0     1\n# … with 1 more variable: niederschlag <dbl>\n\nFor an analysis on rainfall, we need to extract those stations that measure it:\n\n# Get all stations\nniederschlag <- stationen$stations_id[stationen$niederschlag == 1]\n\nIn case you want to automate this script, create a folder where you want to store the data that the {rdwd} package downloads:\n\n# Check if folder for zip data exists & create it if necessary\nif (dir.exists(here::here(path, \"wetter\")) == FALSE) {\n  dir.create(here::here(path, \"wetter\"))}\n\nAfter these initial steps, we can start to download and process recent rainfall data from all stations involved. I download the data on a daily resolution first. Doing so, we can simply count the days since the last rainfall:\n\n# Get URLs for DWD zip files\ndownload <- rdwd::selectDWD(id = niederschlag, \n                            res = \"daily\", \n                            outvec = TRUE, \n                            var = \"more_precip\", \n                            per = \"recent\")\n\n# Download actual zip files and extract data as lists\nres1 <- rdwd::dataDWD(url = download, \n                      dir = here::here(path, \"wetter\"), \n                      force = TRUE, \n                      quiet = TRUE, \n                      overwrite = TRUE)\n\n# Create general data frame from all lists\nres1 %>% purrr::map_dfr(as.data.frame) -> res2\nres2 %>% filter(MESS_DATUM > as.POSIXct(\"2020-01-01\")) -> res2\n\n# Process and find last day of niederschlag\nresults <- res2 %>% group_by(STATIONS_ID) %>% \n  mutate(MESS_DATUM = as.POSIXct(MESS_DATUM)) %>% \n  filter(RS > 0) %>% \n  summarise(niederschlag = last(RS), time = last(MESS_DATUM)) %>% \n  mutate(days = as.integer(Sys.Date() - as.Date(time))) %>% \n  arrange(desc(days)) %>% \n  rename(stations_id = STATIONS_ID) %>% \n  mutate(time = format(time, format = \"%d.%m.%Y\"))\n\n# Check if folder with zip files exists and delete if present\nif (dir.exists(here::here(path, \"wetter\")) == TRUE) {\n      unlink(here::here(path, \"wetter\"), recursive = TRUE)\n}\n\nWhat is necessary now is to check whether there has been rainfall on this present day. If so, we have to set the counter of days without rain to 0:\n\nrecent1 <- rdwd::selectDWD(id = niederschlag, \n                           res = \"10_minutes\", \n                           outvec = TRUE, \n                           var = \"precipitation\", \n                           per = \"now\")\n\nif (dir.exists(here::here(path, \"wetter\")) == FALSE) {\n  dir.create(here::here(path, \"wetter\"))}\n\n# Download actual zip files and extract data as lists\nrecent2 <- rdwd::dataDWD(url = recent1, \n                         dir = here::here(path, \"wetter\"), \n                         force = TRUE, \n                         quiet = TRUE, \n                         overwrite = TRUE)\n\n# Create general data frame from all lists\nrecent2 %>% purrr::map_dfr(as.data.frame) -> recent3\n\n# Delete unused columns and delete all data before yesterday to prevent errors\nrecent3 %>% filter(MESS_DATUM > Sys.Date() - 1) %>% \n  dplyr::select(STATIONS_ID, MESS_DATUM, RWS_10) -> recent3\n\n# Wrangle data frame with weather data to get last value\nplausible <- recent3 %>% group_by(STATIONS_ID) %>% \n  summarise(sum = sum(RWS_10)) %>% \n  filter(sum > 0)\n\nif (sum(results$stations_id %in% plausible$STATIONS_ID) != 0) {\n  \n  for (i in seq_along(plausible$STATIONS_ID)) {\n    \n    id <- plausible$STATIONS_ID[i]\n    \n    results$time[results$stations_id == id] <- \n      format(Sys.Date(), format = \"%d.%m.%Y\")\n    \n    results$days[results$stations_id == id] <- 0\n    \n    results$niederschlag[results$stations_id == id] <- \n      plausible$sum[plausible$STATIONS_ID == id]\n    \n    rm(id)\n    \n  }\n  \n}\n\nresults <- results %>% arrange(desc(days))\nresults_regen <- results %>% arrange(desc(niederschlag))\n\nWhat is left now is to create the final data set:\n\n# Get names and station information\nstationen2 <- stationen %>% filter(niederschlag == 1) %>% \n  select(-lufttemperatur, -niederschlag, -wind)\n\n# Join weather dataset with stations dataset to get stations' names\nresults2 <- inner_join(results, stationen2, by = \"stations_id\") %>% \n  select(stationsname, days, niederschlag, time, stationshoehe) %>% \n  mutate(days = paste(days, \"Tag/en\")) %>% \n  rename(Station = stationsname, \n         `Letzter Niederschlag vor` = days,\n         `Niederschlag (in Litern)` = niederschlag,\n         `Letzter Niederschlag` = time,\n         `Stationshöhe (Meter)` = stationshoehe)\n\nif (dir.exists(here::here(path, \"wetter\")) == TRUE) {\n  unlink(here::here(path, \"wetter\"), recursive = TRUE)\n}\n\nThe result looks like this:\n\nhead(results2)\n# A tibble: 6 x 5\n  Station     `Letzter Niedersch… `Niederschlag (in… `Letzter Nieders…\n  <chr>       <chr>                            <dbl> <chr>            \n1 Dietenheim  8 Tag/en                           4   07.06.2021       \n2 Ihringen    7 Tag/en                          13.9 08.06.2021       \n3 Konstanz    7 Tag/en                          20.7 08.06.2021       \n4 Stuttgart … 7 Tag/en                           0.7 08.06.2021       \n5 Aulendorf-… 7 Tag/en                           0.8 08.06.2021       \n6 Baden-Bade… 6 Tag/en                           2.1 09.06.2021       \n# … with 1 more variable: Stationshöhe (Meter) <dbl>\n\nAs a data journalist, I sent these data to a Datawrapper chart (which is essentially a table) that displays all weather stations involved, the days since the last rainfall and the amount of rainfall the last time it rained:\n\n# Capture output which tells the URL of the chart\ncapture.output(dw_data_to_chart(x = results2, chart_id = \"abcdef\"), \n               file = \"/dev/null\")\n\n## Send data to datawrapper\nlog <- capture.output(dw_publish_chart(chart_id = \"abcdef\"))\n\nOn a side note, my R script triggers a python script on my Raspberry Pi if an error occurred and no valid URL was put out; the script will send an E-mail notifying me of the error:\n\n# If there was no valid URL in the output, trigger python script.\nif (grepl(pattern = \"https://datawrapper.dwcdn.net/abcdef\", log[6]) == FALSE) {\n\n    system(\"python3 ~Documents/scripts/duerre.py\")\n\n} else {\n\n    print(paste0(log[6], \" //// \", Sys.time()))\n\n}\n\nThis whole script runs on my Raspberry Pi 3 using a Cronjob.\n\n\n",
    "preview": {},
    "last_modified": "2021-06-15T19:53:30+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-27-covid-19-see-german-shopping-streets-emptying/",
    "title": "Covid-19 - See German shopping streets emptying",
    "description": "The coronavirus is halting public live. Looking at pedestrian data shows how persistently people stay away from German city centres and give an estimate of the situation is for shops in city centres.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2020-03-28",
    "categories": [],
    "contents": "\nThe coronavirus is halting public live all over the world. So, many of the patterns of data regarding flight departures or traffic that we see these days are, objectively speaking, unsurprising. As of now, most of us are coping well with the situation and stay at home, as is recommended. As a result, the public sphere is emptying. Right now, this is a strategy that I support. However, it is hitting the economy harder than one might think. This is especially true for the small shops in the city centres. Most of them have been forced to close.\nSo, looking at pedestrian data can show us two things, anyway. 1) They show us how persistently people are keeping away from German city centres. And 2) they show us a rough estimate of how bad the situation is for the shops in the city centres. Maybe two years ago, I learned from a colleague that there is this new homepage where one could get pedestrian data from German cities, where they measure the amount of pedestrians on central German shopping streets using lasers. The site is called hystreet.com and its data can be downloaded freely after creating an account. Since the spread of Covid-19, some journalists have had a look at this kind of data already for the big German cities. In this tiny, quick hands-on analysis, I focus on the federal state of Baden-Württemberg, which is where I live.\nFirst, we need a couple of packages.\n\n\nlibrary(tidyverse, warn.conflicts = FALSE)\nlibrary(here)\nlibrary(lubridate)\n\n\n\nSecond, we need to do a bit of data wrangling. I start with loading two datasets that I downloaded from hystreet.com. They contain data for the largest shopping street in Stuttgart, the Königstraße. The dataset from 2020 contains data from January 6th until March 26th. I have a second one from 2019 that contains data from March 1st to March 31st.\n\n\n# Read the csv file from hystreet.com\n\npath <- paste0(here::here(), \n              \"/_posts/2021-06-27-covid-19-see-german-shopping-streets-emptying/\")\n\ndf1 <- read.csv(here::here(path, \"stuttgart-königstraße-mitte.csv\"), \n                sep = \";\", \n                stringsAsFactors = FALSE)\ndf2 <- read.csv(here::here(path, \"stuttgart-königstraße-mitte-2019.csv\"), \n                sep = \";\", \n                stringsAsFactors = FALSE)\n\nrm(path)\n\n\n\n\n\n# Some data preparation\ndf1 <- df1 %>% mutate(time_of_measurement = as.POSIXct(time_of_measurement),\n               date = lubridate::as_date(time_of_measurement),\n               time = lubridate::ceiling_date(time_of_measurement, unit = \"hours\"))\n\ndf2 <- df2 %>% mutate(time_of_measurement = as.POSIXct(time_of_measurement),\n               date = lubridate::as_date(time_of_measurement),\n               time = lubridate::ceiling_date(time_of_measurement, unit = \"hours\"))\n\n\n\nAfter doing so, the data set looks like this:\n\n\nglimpse(df1)\n\n\nRows: 30,934\nColumns: 7\n$ location            <chr> \"Königstraße (Mitte), Stuttgart\", \"König…\n$ time_of_measurement <dttm> 2020-01-06 00:04:21, 2020-01-06 00:09:5…\n$ counted_pedestrians <int> 31, 24, 34, 30, 32, 23, 23, 12, 27, 27, …\n$ type                <chr> \"regular\", \"regular\", \"regular\", \"regula…\n$ incidents           <lgl> NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, …\n$ date                <date> 2020-01-06, 2020-01-06, 2020-01-06, 202…\n$ time                <dttm> 2020-01-06 01:00:00, 2020-01-06 01:00:0…\n\nhead(df1, n = 5)\n\n\n                        location time_of_measurement\n1 Königstraße (Mitte), Stuttgart 2020-01-06 00:04:21\n2 Königstraße (Mitte), Stuttgart 2020-01-06 00:09:56\n3 Königstraße (Mitte), Stuttgart 2020-01-06 00:14:15\n4 Königstraße (Mitte), Stuttgart 2020-01-06 00:19:58\n5 Königstraße (Mitte), Stuttgart 2020-01-06 00:24:37\n  counted_pedestrians    type incidents       date\n1                  31 regular        NA 2020-01-06\n2                  24 regular        NA 2020-01-06\n3                  34 regular        NA 2020-01-06\n4                  30 regular        NA 2020-01-06\n5                  32 regular        NA 2020-01-06\n                 time\n1 2020-01-06 01:00:00\n2 2020-01-06 01:00:00\n3 2020-01-06 01:00:00\n4 2020-01-06 01:00:00\n5 2020-01-06 01:00:00\n\nThe first thing I am interested in is the long-term perspective. First restrictions on public life have been announced by the federal government on March 17th (red line in the figure below), further restrictions followed on March 23rd (blue line in the figure below). The second figure shows a comparable time span from 2019. This way, we can (if only slightly) control for seasonal effects, too. That second time span of 2019 serves as a benchmark for how the number of pedestrians would have developed if the coronavirus restrictions would not have hit. I scaled the y axis equally to a maximum of 15,000 pedestrians per hour to spot the difference more easily.\n\n\ndf1 %>% filter(date >= \"2020-03-01\") %>% group_by(time) %>% \n  summarise(n = sum(counted_pedestrians)) %>%\n ggplot(aes(x = time, y = n)) + \n  geom_line(size = 1) +\n  geom_vline(xintercept = as.POSIXct(\"2020-03-17\"), color = \"red\") +\n  geom_vline(xintercept = as.POSIXct(\"2020-03-23\"), color = \"blue\") +\n  ggtitle(\"2020: Pedestrians in the Stuttgart city centre\", \n          subtitle = \"Cumulated number on Königstraße (Mitte)\") + \n  labs(caption = \"Source: hystreet.com\") + \n  xlab(\"Date\") + \n  ylab(\"Sum of pedestrians per hour\") +\n  ylim(0, 15000)\n\n\n\ndf2 %>% filter(date <= \"2019-03-26\") %>% group_by(time) %>% \n  summarise(n = sum(counted_pedestrians)) %>%\n ggplot(aes(x = time, y = n)) + \n  geom_line(size = 1) +\n  ggtitle(\"2019: Pedestrians in the Stuttgart city centre\", \n          subtitle = \"Cumulated number on Königstraße (Mitte)\") + \n  labs(caption = \"Source: hystreet.com\") + \n  xlab(\"Date\") + \n  ylab(\"Sum of pedestrians per hour\") + \n  ylim(0, 15000)\n\n\n\n\nYou can see that even before the restrictions were passed by the federal government, numbers of pedestrians were already in decline and are now almost absent. In raw numbers, this is (again, compared to 2019):\n\n\ndf1 %>% group_by(date) %>% \n  summarise(n = sum(counted_pedestrians)) %>% tail(., n = 7)\n\n\n# A tibble: 7 x 2\n  date           n\n  <date>     <int>\n1 2020-03-20  9908\n2 2020-03-21  2884\n3 2020-03-22  4350\n4 2020-03-23  6628\n5 2020-03-24  7564\n6 2020-03-25  6692\n7 2020-03-26   656\n\ndf2 %>% group_by(date) %>% \n  summarise(n = sum(counted_pedestrians)) %>% tail(., n = 7)\n\n\n# A tibble: 7 x 2\n  date            n\n  <date>      <int>\n1 2019-03-25  34788\n2 2019-03-26  40252\n3 2019-03-27  44181\n4 2019-03-28  50518\n5 2019-03-29  69944\n6 2019-03-30 148604\n7 2019-03-31  35709\n\nAs a last analysis, I want to focus on the weekends, unsurprisingly the days with most pedestrians in the city centres. This last figure basically speaks for itself. I compare two weekends end of March, 2020 versus 2019. It is evident 1) how many people stayed home (as long as they did not meet up in any other place instead) and 2) how many potential customers the city centre shops are missing every day (and especially Saturday) that they have to remain closed.\n\n\ntime1 <- seq.POSIXt(as.POSIXct(\"2020-03-20 00:00:00\"), \n                    as.POSIXct(\"2020-03-23 00:00:00\"), by = \"hour\")\n\ntime2 <- seq.POSIXt(as.POSIXct(\"2019-03-22 00:00:00\"), \n                    as.POSIXct(\"2019-03-25 00:00:00\"), by = \"hour\")\n\nplot1 <- df1 %>%\n   filter(time %in% time1) %>%\n   group_by(time) %>%\n   summarise(n = sum(counted_pedestrians)) %>%\n   mutate(hour = lubridate::hour(time),\n          date = lubridate::as_date(time),\n          day = lubridate::day(time),\n          id = 1:length(n))\n\nplot2 <- df2 %>%\n    filter(time %in% time2) %>%\n    group_by(time) %>%\n    summarise(n = sum(counted_pedestrians)) %>% \n    mutate(hour = lubridate::hour(time),\n          date = lubridate::as_date(time),\n          day = lubridate::day(time),\n          id = 1:length(n))\n\n plot1 %>% ggplot(aes(x = id, y = n)) + geom_line(size = 1, color = \"red\") +\n   ggtitle(\"Pedestrians on a March weekend 2020 (red) versus 2019\",\n           subtitle = \"Königstraße Stuttgart (Mitte)\") +\n   ylim(0, 15000)+\n   ylab(\"Number of pedestrians per hour\") +\n   xlab(\"Time in hours (start: Friday 00:00)\") +\n   geom_line(data = plot2) +\n   labs(caption = \"Source: hystreet.com\")\n\n\n\n\nThis is not the most insightful analysis since the figures that we can see were to expected given the current Covid-19 crisis. However, I think by using data from hystreet.com we can make a bit more evident how many people actually stayed away from the centres. I replicated the analysis for a couple of other cities of south west Germany – unsurprisingly, again, they all look pretty similar. If you continued up until this very end: Stay healthy – and please stay home (for now).\n\n\n\n",
    "preview": "posts/2021-06-27-covid-19-see-german-shopping-streets-emptying/covid-19-see-german-shopping-streets-emptying_files/figure-html5/plots-1.png",
    "last_modified": "2021-06-27T22:47:21+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-27-scraping-spiegel-articles-using-newsanchor/",
    "title": "Scraping Spiegel articles using {newsanchor}",
    "description": "One convenient use of {newsanchor} is to use it to scrape the articles' content. Our package is of great help because it provides you with the corresponding URLs.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2020-01-27",
    "categories": [],
    "contents": "\nAlmost a year ago, we (a team at CorrelAid) published our first open source package for R on CRAN: {newsanchor}. It queries the API of newsapi.org and allows you to easily download the articles relating to your search query of a range of popular international news outlets. Results include a variety of meta data, the URLs as well as (depending on whether you have a paid plan or not) parts of the article content. You can find all information needed reading its vignette. I also wrote an introductory blog post which you can find here.\nOne convenient use of {newsanchor} is to use it to scrape the articles’ content. Our package is of great help because it provides you with the corresponding URLs. It is fairly easy to build a scraper upon that. Here, I have to mention that vast parts of the following code stem from Jan Dix who, like me, co-authored the package. He wrote a scraper for the New York Times, it was originally to be included as a vignette of {newsanchor} (it had to be removed because of dependecy trouble, though). What I did was to build on his code and add another example for the popular German news magazine Spiegel.\n[Note: There is code for a progress bar in the following chunk. It is commented out so the R-Markdown output would be easier to read]\n\n\n# Load packages required\nlibrary(newsanchor) # download newspaper articles\nlibrary(robotstxt)  # get robots.txt\nlibrary(httr)       # http requests\nsuppressMessages(library(rvest))      # web scraping tools\nsuppressMessages(library(dplyr))      # easy data frame manipulation\nlibrary(stringr)    # string/character manipulation \n# Get headlines published by SPON using newsanchor (example)\nresponse <- get_everything_all(query   = \"Merkel\",\n                                 sources = \"spiegel-online\",\n                                 from    = Sys.Date() - 3,\n                                 to      = Sys.Date())\n  \n# Extract response data frame\narticles <- response$results_df\n\n# Check robots.txt if scraping is OK\nsuppressMessages(allowed <- paths_allowed(articles$url))\nall(allowed)\n\n# Define parsing function\nget_article_body <- function (url) {\n  \n  # Download article page\n  response <- GET(url)\n  \n  # Check if request was successful\n  if (response$status_code != 200) return(NA)\n  \n  # Extract HTML\n  html <- content(x        = response, \n                  type     = \"text\", \n                  encoding = \"UTF-8\")\n  \n  # Parse html\n  parsed_html <- read_html(html)                   \n  \n  # Define paragraph DOM selector\n  selector <- \"div.clearfix p\"\n  \n  # Parse content\n  parsed_html %>% \n    html_nodes(selector) %>%      # extract all paragraphs within class 'article-section'\n    html_text() %>%               # extract content of the <p> tags\n    str_replace_all(\"\\n\", \"\") %>% # replace all line breaks\n    paste(collapse = \" \")         # join all paragraphs into one string\n}\n\n# Apply function to all URLs\n# Create new text column\narticles$body <- NA\n# Initialize progress bar\n# pb <- txtProgressBar(min     = 1, \n#                      max     = nrow(articles), \n#                      initial = 1, \n#                      style   = 3)\n\n# Loop through articles and apply function\nfor (i in 1:nrow(articles)) {\n  \n  # Apply function to i in URLS\n  articles$body[i] <- get_article_body(articles$url[i])\n  \n  ## Update progress bar\n  # setTxtProgressBar(pb, i)\n  \n  # Sleep for 1 sec\n  Sys.sleep(1)\n}\n\n\n\nBased on the articles’ content, you can, for example, compute sentiment analyses. Jan shows you how to do that for the New York Times in what was formerly our vignette here. I hope this is useful for some of you.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-28T17:28:22+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-23-automatically-filling-a-sqlite-database-using-aws-and-rstudio-server/",
    "title": "Automatically filling a SQLite database using AWS and RStudio Server",
    "description": "The R script used is a simple parser. To automate it, set up a SQLite database on AWS. The script parses a JSON file and writes it into the database using SQL.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-05-12",
    "categories": [],
    "contents": "\nFor about two years now, I measure the air quality (more specifically: fine dust) on my balcony. Up to now, the data were stored by an Open Data project and displayed (more or less hidden) in the internet – at least you would need to know the name or ID of the sensor to access the data as very basic graphs. Here you could also find aggregated air quality data for all areas of Stuttgart. Recently, I began storing the data into a database myself to be able to do my own calculations (and eventually create a Shiny App that would display nicer, interactive graphs). Here, I wanted to very quickly write down what I did and what is going on.\nThe sensor\nThe sensor measuring fine dust is a very basic one. All its components cost around 30€ and can be shopped online. Honestly, I did not build the sensor myself; a colleague had a spare one and gave it to me. However, I was told it is not too hard to build such a sensor. In fact, here you can find a (German only) manual for doing so, including a list of stuff you need for the sensor itself. My sensor is located on my balcony, which in turn faces a highly frequented road, where quite a lot of buses are passing by during the day. The sensor delivers the data it measures every (more or less) five minutes via a JSON file to an API that is open for everyone to read. The sensor itself runs quite smoothly, only recently I run into problems where it ceases to send information – this is fixed after quickly removing it from its power source.\nAmazon AWS with RStudio Server\nWhen I came up with the idea of automatically storing the data into a database, I was initially thinking about setting up a Raspberry Pi to do the job. Yet, a friend recommended using Amazon AWS instead. Frankly, when I did this, I was new to cloud computing. However, there are a lot of tutorials out there on how to set up an Amazon AWS instance. I remember choosing the smallest one (EC2) which is also completely free of charge. That is useful since it is supposed to run 24/7. Whilst setting up the instance, I also installed RStudio Server and Shiny Server. That would later allow me to work on R scripts on the instance in order to do the parsing of the JSON file.\nThe script\nThe R script is nothing more than a simple parser for the JSON file that the API publishes. As a first step, I set up a SQLite database on my AWS instance using the R package sqldf. It contains only three variables: the timestamp of the measurement and the two fine dust measurements that the sensor gives us. The script parses the JSON file (using jsonlite) and writes it into the database using SQL. Here is some example code:\n\n\ndb <- dbConnect(SQLite(), dbname = \"/home/user/database.sqlite\")\nquery1 <- paste0(\"INSERT INTO database VALUES ('\", time1, \"',\", PM_10_1, \",\", PM_25_1, \")\")\ndbSendQuery(conn = db, query1)\n\n\n\nSetting up the Crontab\nThe last step is to automate the R script that we have stored on the AWS instance, too. You can do this using a Crontab, which is an automation tool that is already present on the Linux of the EC2 instance. You can find a tutorial here.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-23T00:16:10+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-22-introducing-the-r-newsanchor-package/",
    "title": "Introducing the R {newsanchor} package",
    "description": "At CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs using newsapi.org.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-03-06",
    "categories": [],
    "contents": "\nAt CorrelAid, we developed a tool for communication scientists, journalists and data scientists alike:* {newsanchor}, CorrelAid’s first open source R package. It conveniently helps you to access breaking news and articles from over 30,000 news sources and blogs - using the API of newsapi.org. I have co-authored the package and I am thrilled to introduce the package here, too.\nThe (mostly free) News API is one way to access text as a resource for data analyses. It provides news articles and breaking news from a variety of sources across various countries, delivered to the analyst via an API (HTTP REST). Users are offered three API endpoints: top headlines, everything, and sources. Top headlines provides access to live breaking headlines of the news sources in a given country. Everything outputs articles published by these sources on a specified search query, even back in the past. Sources helps users to get access to the set of news sources that are available to top headlines.\nAll search requests come with different meta data (URL, author, date of publication, topic, etc.) and can be refined by a huge variety of additional parameters (sources, topic, time, relevance, etc.). For more details, see newsapi.org. Note for German scientists and journalists: In Germany, the following sources are available: Spiegel Online, Handelsblatt, Tagesspiegel, Wired, Gründerszene, BILD, Zeit Online, Focus Online, t3n and Wirtschaftswoche.\nAfter a short registration, the API can be accessed via code: through client libraries such as JavaScript or Ruby. But until now, there has been no R package that does the work (or search) conveniently. Now, at CorrelAid, a team of five data analysts developed this package. The package is called {newsanchor} and is available on CRAN: install.packages(\"newsanchor\").\nNewsanchor provides three functions that correspond to the API’s endpoints: get_headlines(), get_everything() and get_sources(). They help users to conveniently scrape the resources of News API, specify different search parameters and obtain results as 1) a data frame with results and 2) a list with meta data on the search. We also provide comprehensive error messages to make troubleshooting easy. You find details on the usage of newsanchor and its core functions in our general CRAN vignette.\nAnother reason for us to develop the package was that analyses based on words are becoming increasingly important. Political scientists, for example, classify parties on any ideological dimension using party manifestos. Other scholars focus on news articles to extract the (political) framings of the texts. Using automatisation, it is, for example, possible to calculate the sentiment of a given text fragment such as, for instance, online commentaries. The resulting data prove useful both as a dependent variable as well as an independent variable of any further analysis.\nThe importance of text analyses arises from the origin of ‘texts’: People aim at a certain reaction of their readers. Among the producers of texts with most influence are the media: newspapers, online magazines or blogs. By publishing articles, opinion pieces and analyses, they shape public opinion. The topics they choose (or not choose), the words they use, the quantity of articles on a certain issue – all these factors make them a worthy basis of investigation.\nAs already mentioned, an example would be to calculate the sentiment of news articles. Newsanchor can help to filter and scrape texts from news sources. In our example code, our co-developer Jan Dix shows you how to do so by getting URLs of the New York Times with newsanchor::get_everything(), subsequently scraping them with {httr} and analysing the articles’ sentiments.\nWe hope {newsanchor} will help scientists, journalists and other data enthusiasts to start scraping and using text data based on news articles.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-23T00:16:38+02:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-27-voting-behaviour-in-the-2014-scottish-independence-referendum/",
    "title": "Voting Behaviour in the 2014 Scottish Independence Referendum",
    "description": "I investigated the voting behaviour of the Scottish electorate in the 2014 Scottish independence referendum. More precisely, I was the first attempting to explain voting behaviour in the referendum based on a multivariate quantitative approach.",
    "author": [
      {
        "name": "Yannik",
        "url": "yannikbuhl.de"
      }
    ],
    "date": "2019-01-09",
    "categories": [],
    "contents": "\nFor my Master’s thesis, submitted in July 2017, I investigated the voting behaviour of the Scottish electorate in the 2014 Scottish independence referendum. More precisely, I was the first attempting to explain voting behaviour in the referendum based on a multivariate quantitative approach. In doing so, I additionally focused on Scots’ attitudes towards the European Union and how these attitudes influence(d) their behaviour at the ballots. Whilst the existing literature on the referendum mostly ignored the European dimension of the independence debate, my results suggest that the Scots did in fact somehow think about the future of Scotland within or outside of the EU when the cast their vote – and that these attitudes did have an effect on how they would decide.\nIn sum, my results are the following (ceteris paribus): Scots who found EU membership to be a good thing and who thought that an independent Scotland would not be able to remain in the EU had a comparatively low probability of voting in favour of independence. However, once they believed that independent Scotland would be able to remain an EU member, the probability of voting in favour of independence increased sharply. These effects (based on logistic regression controlling for all sorts of variables and attitudes such as age, gender, nationalism, party identificiation, etc.) become more easy to understand once they are presented in a graph using predicted probabilities (higher values of “Perceived Likelihood of Scotland retaining EU membership” mark higher confidence of the survey respondent): This relationship gets even more pronounced after computing an interaction effect and separating between proponents and opponents of EU membership: \nThis blog post was just meant to be as short presentation of my results. Here you can download the PDF with the full thesis.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-27T21:57:59+02:00",
    "input_file": {}
  }
]
